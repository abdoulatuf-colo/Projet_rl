{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Allocation équitable des ressources en edge computing (RL)\n",
        "\n",
        "Ce notebook présente un **benchmark minimal et reproductible** de plusieurs techniques d'allocation de ressources (aléatoire, heuristique, DRL) dans un environnement d'edge computing. L'objectif est d'optimiser la **latence moyenne** tout en améliorant l'**équité inter-utilisateurs**.\n",
        "\n",
        "**Techniques évaluées :**\n",
        "- Politique aléatoire (baseline)\n",
        "- Heuristique `min-latence` (baseline)\n",
        "- DQN (Stable-Baselines3)\n",
        "- PPO (Stable-Baselines3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports et configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from stable_baselines3 import DQN, PPO\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environnement Edge Computing (simplifié)\n",
        "L'environnement simule un flux de tâches hétérogènes. À chaque pas, l'agent choisit d'exécuter la tâche sur un serveur **MEC** (proche, rapide mais limité) ou dans le **Cloud** (puissant mais plus distant).\n",
        "\n",
        "**Objectifs :**\n",
        "- Minimiser la latence\n",
        "- Respecter les deadlines\n",
        "- Maintenir l'équité inter-utilisateurs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Task:\n",
        "    user_type: str\n",
        "    workload: float  # millions d'instructions\n",
        "    data_size: float  # MB\n",
        "    deadline: float  # ms\n",
        "    priority: int\n",
        "\n",
        "\n",
        "class EdgeResourceEnv(gym.Env):\n",
        "    metadata = {\"render.modes\": [\"ansi\"]}\n",
        "\n",
        "    def __init__(self, episode_length: int = 200):\n",
        "        super().__init__()\n",
        "        self.episode_length = episode_length\n",
        "        self.step_count = 0\n",
        "\n",
        "        # Action: 0 = MEC, 1 = Cloud\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "        # Observation: [mec_load, cloud_load, workload, data_size, deadline, priority, fairness]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0, 1, 1, 0], dtype=np.float32),\n",
        "            high=np.array([1, 1, 5000, 200, 500, 5, 1], dtype=np.float32),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.user_types = [\"IoT\", \"Mobile\", \"Vehicle\", \"AR_VR\", \"Industrial\"]\n",
        "        self.user_weights = [0.4, 0.3, 0.15, 0.1, 0.05]\n",
        "        self.satisfaction: Dict[str, List[int]] = {u: [] for u in self.user_types}\n",
        "\n",
        "        self.mec_capacity = 100  # ressources arbitraires\n",
        "        self.cloud_capacity = 500\n",
        "        self.mec_load = 0.2\n",
        "        self.cloud_load = 0.4\n",
        "        self.current_task: Task | None = None\n",
        "\n",
        "    def _sample_task(self) -> Task:\n",
        "        user_type = np.random.choice(self.user_types, p=self.user_weights)\n",
        "        profiles = {\n",
        "            \"IoT\": (200, 1.0, 150, 2),\n",
        "            \"Mobile\": (800, 5.0, 120, 3),\n",
        "            \"Vehicle\": (3000, 30.0, 30, 5),\n",
        "            \"AR_VR\": (2000, 50.0, 40, 4),\n",
        "            \"Industrial\": (4000, 80.0, 80, 5),\n",
        "        }\n",
        "        workload, data_size, deadline, priority = profiles[user_type]\n",
        "        return Task(\n",
        "            user_type=user_type,\n",
        "            workload=workload * np.random.uniform(0.8, 1.2),\n",
        "            data_size=data_size * np.random.uniform(0.7, 1.3),\n",
        "            deadline=deadline * np.random.uniform(0.8, 1.2),\n",
        "            priority=priority,\n",
        "        )\n",
        "\n",
        "    def _estimate_latency(self, action: int, task: Task) -> float:\n",
        "        # Latence réseau + traitement\n",
        "        if action == 0:  # MEC\n",
        "            net = 5 + 30 * self.mec_load\n",
        "            proc = (task.workload / (self.mec_capacity * (1 - self.mec_load)))\n",
        "        else:  # Cloud\n",
        "            net = 25 + 40 * self.cloud_load\n",
        "            proc = (task.workload / (self.cloud_capacity * (1 - self.cloud_load)))\n",
        "        data_delay = task.data_size / 5.0\n",
        "        return net + proc + data_delay\n",
        "\n",
        "    def _jain_fairness(self) -> float:\n",
        "        values = []\n",
        "        for user_type in self.user_types:\n",
        "            if self.satisfaction[user_type]:\n",
        "                values.append(np.mean(self.satisfaction[user_type]))\n",
        "        if not values:\n",
        "            return 0.0\n",
        "        values = np.array(values)\n",
        "        return (values.sum() ** 2) / (len(values) * (values ** 2).sum() + 1e-6)\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        task = self.current_task\n",
        "        fairness = self._jain_fairness()\n",
        "        return np.array(\n",
        "            [\n",
        "                self.mec_load,\n",
        "                self.cloud_load,\n",
        "                task.workload,\n",
        "                task.data_size,\n",
        "                task.deadline,\n",
        "                task.priority,\n",
        "                fairness,\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.step_count = 0\n",
        "        self.mec_load = 0.2\n",
        "        self.cloud_load = 0.4\n",
        "        self.satisfaction = {u: [] for u in self.user_types}\n",
        "        self.current_task = self._sample_task()\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        task = self.current_task\n",
        "        latency = self._estimate_latency(action, task)\n",
        "        success = latency <= task.deadline\n",
        "\n",
        "        # Reward: pénalise la latence et les échecs, encourage l'équité\n",
        "        reward = -latency / 100.0\n",
        "        if not success:\n",
        "            reward -= 2.0\n",
        "        else:\n",
        "            reward += 1.0\n",
        "\n",
        "        self.satisfaction[task.user_type].append(int(success))\n",
        "        fairness = self._jain_fairness()\n",
        "        reward += 0.5 * fairness\n",
        "\n",
        "        # Mise à jour des charges\n",
        "        load_delta = 0.03 if action == 0 else 0.02\n",
        "        if action == 0:\n",
        "            self.mec_load = np.clip(self.mec_load + load_delta, 0.05, 0.95)\n",
        "            self.cloud_load = np.clip(self.cloud_load - 0.01, 0.1, 0.9)\n",
        "        else:\n",
        "            self.cloud_load = np.clip(self.cloud_load + load_delta, 0.1, 0.95)\n",
        "            self.mec_load = np.clip(self.mec_load - 0.01, 0.05, 0.9)\n",
        "\n",
        "        self.step_count += 1\n",
        "        terminated = self.step_count >= self.episode_length\n",
        "        self.current_task = self._sample_task()\n",
        "        info = {\"latency\": latency, \"success\": success, \"fairness\": fairness}\n",
        "        return self._get_obs(), reward, terminated, False, info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Baselines et évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_policy(_obs):\n",
        "    return np.random.randint(0, 2)\n",
        "\n",
        "\n",
        "def heuristic_policy(obs):\n",
        "    mec_load, cloud_load, workload, data_size, deadline, priority, fairness = obs\n",
        "    # Choix MEC si la latence estimée est inférieure au seuil et charge modérée\n",
        "    mec_latency = 5 + 30 * mec_load + workload / (100 * (1 - mec_load)) + data_size / 5.0\n",
        "    cloud_latency = 25 + 40 * cloud_load + workload / (500 * (1 - cloud_load)) + data_size / 5.0\n",
        "    if mec_latency <= deadline and mec_load < 0.8:\n",
        "        return 0\n",
        "    return 1 if cloud_latency < mec_latency else 0\n",
        "\n",
        "\n",
        "def evaluate_policy(env, policy_fn, episodes=30):\n",
        "    metrics = {\"latency\": [], \"success\": [], \"fairness\": []}\n",
        "    for _ in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy_fn(obs)\n",
        "            obs, reward, done, _, info = env.step(action)\n",
        "            metrics[\"latency\"].append(info[\"latency\"])\n",
        "            metrics[\"success\"].append(info[\"success\"])\n",
        "            metrics[\"fairness\"].append(info[\"fairness\"])\n",
        "    return {\n",
        "        \"latency_ms\": float(np.mean(metrics[\"latency\"])),\n",
        "        \"success_rate\": float(np.mean(metrics[\"success\"])),\n",
        "        \"fairness\": float(np.mean(metrics[\"fairness\"])),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Entraînement DRL (DQN / PPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = EdgeResourceEnv()\n",
        "\n",
        "dqn_model = DQN(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    learning_rate=3e-4,\n",
        "    buffer_size=5000,\n",
        "    batch_size=64,\n",
        "    gamma=0.98,\n",
        "    verbose=0,\n",
        "    seed=SEED,\n",
        ")\n",
        "dqn_model.learn(total_timesteps=5000)\n",
        "\n",
        "ppo_model = PPO(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    learning_rate=3e-4,\n",
        "    n_steps=512,\n",
        "    gamma=0.98,\n",
        "    verbose=0,\n",
        "    seed=SEED,\n",
        ")\n",
        "ppo_model.learn(total_timesteps=5000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Benchmark comparatif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_env = EdgeResourceEnv()\n",
        "\n",
        "results = []\n",
        "results.append({\"method\": \"Random\", **evaluate_policy(eval_env, random_policy)})\n",
        "results.append({\"method\": \"Heuristic\", **evaluate_policy(eval_env, heuristic_policy)})\n",
        "results.append({\"method\": \"DQN\", **evaluate_policy(eval_env, lambda obs: dqn_model.predict(obs, deterministic=True)[0])})\n",
        "results.append({\"method\": \"PPO\", **evaluate_policy(eval_env, lambda obs: ppo_model.predict(obs, deterministic=True)[0])})\n",
        "\n",
        "pd.DataFrame(results).sort_values(by=\"latency_ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusion (à compléter)\n",
        "- Analysez le compromis **latence / équité / robustesse**\n",
        "- Discutez l'impact de la charge MEC vs Cloud\n",
        "- Proposez des pistes d'amélioration (multi-agent, priorités, contraintes énergétiques)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}